{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion # Azure OpenAI — it abstracts calling your deployment\n",
    "\n",
    "from semantic_kernel.contents.chat_history import ChatHistory # manages a chat thread — each user/assistant message gets stored here so the model can keep context\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,       # execution settings that tell the model how to respond\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d96f166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your Azure OpenAI key from .env\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add your Azure OpenAI service\n",
    "chat_completion = AzureChatCompletion(\n",
    "        deployment_name=\"gpt-4o-mini\",  # Replace with your deployment name\n",
    "        endpoint=\"https://exquitech-openai-2.openai.azure.com/\",\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    )\n",
    "\n",
    "kernel.add_service(chat_completion, \"chat_completion\")          # Adds the model to the kernel under the label \"chat_completion\" so it can be referred to later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "503ee56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt execution settings\n",
    "settings = AzureChatPromptExecutionSettings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a073a",
   "metadata": {},
   "source": [
    "This configures how the model responds. You can customize things here like:\n",
    "\n",
    "- max_tokens\n",
    "\n",
    "- temperature\n",
    "\n",
    "- top_p\n",
    "\n",
    "- frequency_penalty, etc.\n",
    "\n",
    "Default settings are used if you don't set anything — that's okay for basic cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5de1f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat history (context for the model)\n",
    "chat = ChatHistory()\n",
    "chat.add_user_message(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9f21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Get the response from the AI\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat,\n",
    "    settings=settings,\n",
    "    kernel=kernel,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043e037",
   "metadata": {},
   "source": [
    "This line:\n",
    "- Uses your chat_completion service.\n",
    "- Sends the current chat history.\n",
    "- Uses the execution settings.\n",
    "- References the kernel so services and plugins can be pulled in if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a93741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Assistant > \" + str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a23734aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the message from the agent to the chat history\n",
    "chat.add_message(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e27f9a2",
   "metadata": {},
   "source": [
    "- Adds the assistant’s message to the chat history so future calls can maintain context.\n",
    "- If you ask another question like “What is its population?”, the model will know you meant Paris because the chat history includes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15660b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant > As of my last update in October 2021, the population of Paris was approximately 2.1 million people within the city proper. However, the larger metropolitan area, known as the Île-de-France region, has a population of around 12 million. For the most current population figures, I recommend checking the latest statistics from a reliable source, such as the French government's official statistics office or recent census data.\n"
     ]
    }
   ],
   "source": [
    "chat.add_user_message(\"What is the population?\")\n",
    "\n",
    "result = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat,\n",
    "    settings=settings,\n",
    "    kernel=kernel,\n",
    ")\n",
    "\n",
    "print(\"Assistant > \" + str(result))\n",
    "\n",
    "chat.add_message(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d4b47",
   "metadata": {},
   "source": [
    "As you can see, adding the message (result) to the chat (ChatHistory()) after first question, allows multi-turn memory "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
